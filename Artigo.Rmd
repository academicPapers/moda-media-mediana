---
title: "Avaliação pela Moda, Média ou Mediana?"
author:
- Luiz Fernando Palin Droubi
- Willian Zonato
date: "`r format(Sys.Date(), '%d/%m/%Y')`"
output:
  html_document:
    fig_caption: yes
    keep_md: yes
  pdf_document:
    keep_tex: yes
    number_sections: yes
classoption: a4paper
header-includes:
- \usepackage[brazil]{babel}
- \usepackage{graphicx}
- \usepackage{float}
- \usepackage{subfig}
- \usepackage{caption}
documentclass: article
link-citations: yes
csl: ABNT_UFPR_2011-Mendeley.csl
subtitle: Estudo de Caso
bibliography: bibliography.bib
params:
  Nsim: 500
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dev = "png", dpi = 600, out.width = "70%", fig.pos = "H",
                      fig.path = "images/", fig.align = "center", warning = FALSE)
options(digits = 10)
brformat <- function(x, decimal.mark = ",", big.mark = ".", digits = 2, nsmall = 2, scientific = FALSE, ...) {
  format(x, decimal.mark = decimal.mark, big.mark = big.mark, digits = digits, 
         nsmall = nsmall, scientific = scientific, ...)
}
reais <- function(prefix = "R$", ...) {
  function(x) paste(prefix, brformat(x, ...), sep = "")
}
porcento <- function (x) {
    if (length(x) == 0) 
        return(character())
    x <- plyr::round_any(x, scales:::precision(x)/100)
    paste0(x * 100, "\\%")
}
# library(appraiseR)
library(ggplot2)
```

# INTRODUÇÃO

# REVISÃO BIBLIOGRÁFICA

## Esperança matemática ou Valor Esperado

Segundo @wiki:E, a "**esperança matemática** de uma variável aleatória é a soma do produto de cada probabilidade de saída da experiência pelo seu respectivo valor. Isto é, representa o valor médio 'esperado' de uma experiência se ela for repetida muitas vezes". Matematicamente, a Esperança de uma variável aleatória $X$ é representada pelo símbolo $E[X]$, de tal forma que, pela definição dada acima, no caso de uma variável aleatória discreta:

$$E[X] = \sum_{i = 1}^{\infty}x_ip(x_i)$$

Já para uma variável aleatória contínua, o valor esperado torna-se:

$$E[X] = \int_{-\infty}^{\infty}xf(x)dx$$


## O problema da retransformação das variáveis

### Regressão Linear

De acordo com Duan [-@Duan, 606], o Valor Esperado $E$ de uma variável resposta $Y$ que tenha sido transformada em valores $\eta$ durante a regressão linear por uma função $g(Y)$ **não-linear** não é igual ao valor da simples retransformação da variável transforma pela sua função inversa $h(\eta) = g^{-1}(Y)$. Em outros termos[@Duan, 606]:

$$E[Y_0] = E[h(x_0\beta + \epsilon)] \ne h(x_o\beta)$$

Numa regressão linear logaritmizada, ou seja, uma regressão linear com o logarítmo da variável dependente ($h(\eta) = g^{-1}(\eta) = exp(\eta)$), para efetuar apropriadamente a retransformação das estimativas de volta a sua escala original, precisa-se ter em conta a desigualdade mencionada na seção \ref{esperanca-matematica-ou-valor-esperado}.

Segundo [@NBERt0246], quando ajustamos o logaritmo natural de uma variável $Y$ contra outra variável $X$ através da seguinte equação de regressão:

$$ln(Y) = \beta_0 + \beta_1X + \epsilon$$

De acordo com [@NBERt0246, 6; @Duan, 606], se o erro $\epsilon$ é normalmete distribuído, com média zero e desvio padrão $\sigma^2$, ou seja, se $\epsilon \sim N(0, \sigma^2)$, então:

$$E[Y|X] = e^{\beta_0 + \beta_1X} \cdot E[e^\epsilon] \ne e^{\beta_0 + \beta_1X}$$

Embora o valor esperado dos resíduos $\epsilon$ seja igual a zero, ele está submetido a uma transformação não linear, de maneira que não podemos afirmar que $E[e^\epsilon] = 1$, como vimos na seção anterior. Desta maneira, o estimador abaixo é enviesado:

$$E[Y|X] = e^{\beta_0 + \beta_1X}$$

Porém se o termo de erro $\epsilon$ é normalmente distribuído $N(0,\sigma^2)$, então um estimador não-enviesado para o valor esperado $E[Y]$, de acordo com @Duan, assume a forma vista na equação abaixo[@Duan, 606; @NBERt0246, 2 e 6]:

$$E[Y] = e^{\beta_0 + \beta_1X} \cdot e^{\frac{1}{2}\sigma^2}$$

### Modelo linear generalizado (*GLM*)

De acordo com [@NBERt0246, 3-4], um modelo linear generalizado com uma função de ligação logarítimica estimam $log(E[Y|X])$ diretamente, de tal maneira que:

$$log(E[y|X]) = x\beta$$ ou
$$E[Y|X] = e^{x\beta}$$

# ESTUDO DE CASO

Neste estudo comparamos a precisão de diversos tipos de modelos estatísticos (regressão linear, regressão não-linear e modelo linear generalizado) sobre dados gerados com erros randômicos normais com média zero e desvio-padrão $\sigma = 1$.

## Geração de dados randômicos

Para a geração de dados foi utilizada a seguinte expressão teórica, dentro do intervalo $0 \leqslant x \leqslant  1$:

$$y = e^{-5x + 2}$$
Para obter alguma variabilidade, foram adicionados aos valores teóricos de $y$ erros normais $N(0;0,2)$.

```{r dados}
set.seed(123)
Nsim <- params$Nsim

a = -5
b = 2

x = runif(Nsim, 0, 1)
y = exp(a*x + b + rnorm(Nsim, 0, .2))
```

* Gráfico dos dados gerados

```{r grafico, fig.cap = "Gráfico dos dados gerados"}
plot(x,y, pch = 16, cex = 0.5)
```


### Gráfico da variável transformada

```{r graficotrans, fig.cap = "Gráfico da variável transformada", fig.keep='last'}
plot(x, log(y), pch = 16, cex = 0.5) 
abline(lm(log(y) ~ x), col = 2)
```

## Ajuste da regressão não-linear

```{r nls}
### NLS Fit
NLfit <- nls(y ~ exp(a*x+b), start = c(a = -10, b = 15)) 
```

### Coeficientes

```{r coef}
co <- coef(NLfit)
co
```

### Gráfico do modelo não-linear

```{r graficoNL, fig.cap = "Gráfico do modelo não-linear"}
f <- function(x,a,b) {exp(a*x+b)}
curve(f(x = x, a = co[1], b = co[2]), col = 2, lwd = 1.2) 
curve(f(x = x, a = -5, b = 2), col = 3, lwd = 1.5, add = TRUE)
```

### Estimativas do modelo não-linear

```{r}
pNLfit <- predict(NLfit, newdata = data.frame(x = .7))
pNLfit
```

O valor teórico obtido pela equação original ($y = e^{-5x + 2}$) é de:

```{r}
Yteorico <- exp(-5*.7 + 2)
round(Yteorico, 4)
```

$$\epsilon = \frac{\hat{Y} - Y_{teórico}}{Y_{teórico}}$$

O valor obtido pelo modelo é muito próximo do valor teórico. O erro do modelo, portanto, é de `r porcento((pNLfit - Yteorico)/Yteorico)`.

## Ajuste de modelo linear generalizado

### Poisson

```{r glm}
Gfit <- glm(y ~ x, family = poisson())
summary(Gfit)
```

#### Estimativa com o modelo linear generalizado com Poisson

```{r}
pGfit <- predict(Gfit, newdata = data.frame(x = .7), type = "response")
pGfit
```

O valor obtido pelo modelo também é muito próximo do valor teórico obtido pela equação original ($y = e^{-5x + 2}$). Neste caso, o erro do modelo é de `r porcento((pGfit - Yteorico)/Yteorico)`.

### Gauss

```{r glm2}
Gfit2 <- glm(y ~ x, family = gaussian(link = "log"))
summary(Gfit2)
```

#### Estimativa com o modelo linear generalizado com Gauss

```{r}
pGfit2 <- predict(Gfit2, newdata = data.frame(x = .7), type = "response")
pGfit2
```

O valor obtido pelo modelo também é muito próximo do valor teórico obtido pela equação original ($y = e^{-5x + 2}$). Neste caso, o erro do modelo é de `r porcento((pGfit2 - Yteorico)/Yteorico)`. Observar que a adoção de ajuste por modelo linear generalizado com família gaussiana e *log-link* é equivalente ao ajustamento de um modelo de regressão não-linear, como visto na seção anterior.

## Ajuste de Regressão Linear com variável dependente transformada

```{r lm}
### LM Fit
fit <- lm(log(y) ~ x)
s <- summary(fit)
s
```

### Verificação da normalidade

```{r}
shapiro.test(fit$residuals)
```
```{r}
res <- data.frame(fit$residuals)
ggplot(res, aes(x = fit.residuals)) +
  geom_histogram(aes(y = ..density..), bins = 8) +
  stat_density(geom = "line", aes(colour = "Kernel")) +
  stat_function(fun = dnorm,
                args = list(mean = mean(fit$residuals), sd = sd(fit$residuals)), 
                aes(colour = "Normal")) + 
  theme(legend.position = "bottom", legend.title = element_blank(),
        legend.text = element_text(size = 8))
```


### Gráfico do modelo linear

```{r graficoFIT, fig.cap = "Gráfico do modelo linear"}
#plotmod(fit)
```


### Estimativas

a. Pela mediana

```{r mediana}
Y <- predict(fit, newdata = data.frame(x = .7))
p_mediana <- exp(Y)
p_mediana
```

O erro do modelo, neste caso, é de `r porcento((p_mediana - Yteorico)/Yteorico)`.

b. Pela moda

```{r moda}
p_moda <- exp(Y - s$sigma^2)
p_moda
```

O erro do modelo, neste caso, é de `r porcento((p_moda - Yteorico)/Yteorico)`.

c. Pela média

```{r media}
p_media <- exp(Y + s$sigma^2/2)
p_media
```

O erro do modelo, neste caso, é de `r porcento((p_media - Yteorico)/Yteorico)`.

## Comparação dos resultados obtidos

| Modelo                | Previsão                    | Erro (%)                                    | 
|:----------------------|----------------------------:|--------------------------------------------:|
| **Valor Teórico**     | **`r round(Yteorico, 4)`**  | ------                                      |
| Regressão Não-Linear  | `r round(pNLfit, 4)`        |`r porcento((pNLfit-Yteorico)/Yteorico)`     |
| GLM (Poisson)         | `r round(pGfit, 4)`         |`r porcento((pGfit-Yteorico)/Yteorico)`      |
| GLM (Gauss)           | `r round(pGfit2, 4)`        |`r porcento((pGfit2-Yteorico)/Yteorico)`     |
| LM (Mediana)          | `r round(p_mediana, 4)`     |`r porcento((p_mediana-Yteorico)/Yteorico)`  |
| LM (Moda)             | `r round(p_moda, 4)`        |`r porcento((p_moda-Yteorico)/Yteorico)`     |
| LM (Média)            | `r round(p_media, 4)`       |`r porcento((p_media-Yteorico)/Yteorico)`    |

# Método de Monte-Carlo

O resultados acima não devem ser interpretados como taxativos, pois os valores encontrados foram obtidos de dados gerados randomicamente e em único ponto.

Para uma comparação mais precisa entre os modelos testados, utilizamos o método de Monte Carlo em conjunto com a técnica de validação cruzada, simulando os modelos em apenas parte dos dados (*training set*) e fazendo previsões dos dados na outra particação (*test set*).

Para este caso, vamos dividir randomicamente os dados em duas partições iguais, ou seja, os modelos serão gerados em cima de metade dos dados (*training set*) e as predições serão efetuadas e comparadas aos 50% de dados restantes (*test set*).

## Dados

```{r}
library(readxl)
df <- read_excel("amostra.xlsx")
```

## Simulações

```{r, cache = TRUE}
pNL <- list()
pG <- list()
pG2 <- list()
p_mediana <- list()
p_moda <- list()
p_media <- list()
ASPE_pNL <- vector(mode = "numeric", length = Nsim/2)
ASPE_pG <- vector(mode = "numeric", length = Nsim/2)
ASPE_pG2 <- vector(mode = "numeric", length = Nsim/2)
ASPE_mediana <- vector(mode = "numeric", length = Nsim/2)
ASPE_moda <- vector(mode = "numeric", length = Nsim/2)
ASPE_media <- vector(mode = "numeric", length = Nsim/2)
for (i in seq_len(Nsim)) {
  subset <- sample(Nsim, Nsim/2, replace = FALSE)
  trainingset <- df[subset, ]  
  testset <-  df[-subset, ]
  NLfit <- nls(Y ~ exp(a*X + b), data = trainingset, start = c(a = -10, b = 15)) 
  Gfit <- glm(Y ~ X, family = poisson(), data = trainingset)
  Gfit2 <- glm(Y ~ X, family = gaussian(link = "log"), data = trainingset)
  fit <- lm(log(Y) ~ X, data = trainingset)06
  ASPE_pG2[i] <- sum((pG2[[i]] - testset)^2)
  ASPE_mediana[i] <- sum((p_mediana[[i]] - testset)^2)
  ASPE_moda[i] <- sum((p_moda[[i]] - testset)^2)
  ASPE_media[i] <- sum((p_media[[i]] - testset)^2)
}
mean(ASPE_pNL)
mean(ASPE_pG)
mean(ASPE_pG2)
mean(ASPE_mediana)
mean(ASPE_moda)
mean(ASPE_media)
```

## Gráficos

```{r histogramas, out.width="100%", echo = FALSE, message=FALSE, fig.cap = "Histogramas das variáveis simuladas"}
data <- data.frame(ASPE_pNL, ASPE_pG, ASPE_pG2, ASPE_mediana, ASPE_moda, ASPE_media)
p <- list()
p[[1]] <- ggplot(data, aes(x = ASPE_pNL), breaks = 10) + 
  geom_histogram(aes(y = ..density..)) + 
  stat_density(geom = "line", aes(colour = "Kernel")) +
  stat_function(fun = dnorm,
                args = list(mean = mean(data$ASPE_pNL), sd = sd(data$ASPE_pNL)), 
                aes(colour = "Normal")) + 
  theme(legend.position = "bottom", legend.title = element_blank(),
        legend.text = element_text(size = 8))
p[[2]] <- ggplot(data, aes(x = ASPE_pG), breaks = 10) + 
  geom_histogram(aes(y = ..density..)) + 
  stat_density(geom = "line", aes(colour = "Kernel")) +
  stat_function(fun = dnorm, 
                args = list(mean = mean(data$ASPE_pG), sd = sd(data$ASPE_pG)), 
                aes(colour = "Normal")) + 
  theme(legend.position = "bottom", legend.title = element_blank(),
        legend.text = element_text(size = 8))
p[[3]] <- ggplot(data, aes(x = ASPE_pG2), breaks = 10) + 
  geom_histogram(aes(y = ..density..)) + 
  stat_density(geom = "line", aes(colour = "Kernel")) +
  stat_function(fun = dnorm, 
                args = list(mean = mean(data$ASPE_pG2), sd = sd(data$ASPE_pG2)), 
                aes(colour = "Normal")) + 
  theme(legend.position = "bottom", legend.title = element_blank(),
        legend.text = element_text(size = 8))
p[[4]] <- ggplot(data, aes(x = ASPE_mediana), breaks = 10) + 
  geom_histogram(aes(y = ..density..)) + 
  stat_density(geom = "line", aes(colour = "Kernel")) +
  stat_function(fun = dnorm, 
                args = list(mean = mean(data$ASPE_mediana), sd = sd(data$ASPE_mediana)), 
                aes(colour = "Normal")) + 
  theme(legend.position = "bottom", legend.title = element_blank(),
        legend.text = element_text(size = 8))
p[[5]] <- ggplot(data, aes(x = ASPE_moda), breaks = 10) + 
  geom_histogram(aes(y = ..density..)) + 
  stat_density(geom = "line", aes(colour = "Kernel")) +
  stat_function(fun = dnorm, 
                args = list(mean = mean(data$ASPE_moda), sd = sd(data$ASPE_moda)), 
                aes(colour = "Normal")) + 
  theme(legend.position = "bottom", legend.title = element_blank(),
        legend.text = element_text(size = 8))
p[[6]] <- ggplot(data, aes(x = ASPE_media), breaks = 10) + 
  geom_histogram(aes(y = ..density..)) + 
  stat_density(geom = "line", aes(colour = "Kernel")) +
  stat_function(fun = dnorm, 
                args = list(mean = mean(data$ASPE_media), sd = sd(data$ASPE_media)), 
                aes(colour = "Normal")) + 
  theme(legend.position = "bottom", legend.title = element_blank(),
        legend.text = element_text(size = 8))
cowplot::plot_grid(plotlist = p, ncol = 3)
```




# REFERÊNCIAS {-}